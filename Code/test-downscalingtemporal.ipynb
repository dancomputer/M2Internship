{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from dask.distributed import Client\n",
    "import dask.delayed\n",
    "import time\n",
    "import pandas as pd \n",
    "import dask\n",
    "import os\n",
    "from dask.distributed import LocalCluster\n",
    "#os.chdir(\"/tank/crop_modelling/Daniel/NecessaryM1InternshipCode/ProjectRice\")\n",
    "\n",
    "#start cluster\n",
    "cluster = LocalCluster(n_workers=os.cpu_count() - 5 ,threads_per_worker=1)          # Fully-featured local Dask cluster: forward port 8787 to via vscode to see dashboard\n",
    "client = cluster.get_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_netcdf_data(yield_path, maturity_path):\n",
    "    \"\"\"\n",
    "    Processes NetCDF files in a folder, extracts yield and maturity data, and creates a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        yield_path: The path to the folder containing the yield .nc files.\n",
    "        maturity_path: The path to the folder containing the maturity .nc files.\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame containing the processed data. Returns an empty DataFrame if no .nc files are found.\n",
    "    \"\"\"\n",
    "    yield_files = [f for f in os.listdir(yield_path) if f.endswith(\".nc\")]\n",
    "    maturity_files = [f for f in os.listdir(maturity_path) if f.endswith(\".nc\")]\n",
    "\n",
    "    if not yield_files:\n",
    "        print(\"No .nc files found in the specified yield folder.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame\n",
    "\n",
    "    # Create a dictionary to store parsed metadata of maturity files\n",
    "    maturity_dict = {}\n",
    "\n",
    "    for file_name in maturity_files:\n",
    "        parts = file_name[:-3].split(\"_\")  # Remove .nc and split\n",
    "        if len(parts) != 10:\n",
    "            continue  # Skip files with incorrect formatting\n",
    "\n",
    "        model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, variable_crop_irrigation, region, time_step, start_year, end_year = parts\n",
    "        variable, crop_irrigation = variable_crop_irrigation.split(\"-\", 1)\n",
    "        crop, irrigation = crop_irrigation.rsplit(\"-\", 1)\n",
    "\n",
    "        # Store the file path using parsed metadata as key\n",
    "        key = (model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, crop, irrigation, region, time_step, start_year, end_year)\n",
    "        maturity_dict[key] = os.path.join(maturity_path, file_name)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for file_name in yield_files:\n",
    "        try:\n",
    "            yield_file_path = os.path.join(yield_path, file_name)\n",
    "            ds_yield = xr.open_dataset(yield_file_path, decode_times=False)\n",
    "\n",
    "            # Extract yield variable\n",
    "            variable_names = list(ds_yield.data_vars)\n",
    "            if not variable_names:\n",
    "                print(f\"Warning: No data variables found in {file_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            yield_variable_name = variable_names[0]\n",
    "            yield_data = ds_yield[yield_variable_name]\n",
    "\n",
    "            # Parse filename\n",
    "            parts = file_name[:-3].split(\"_\")\n",
    "            if len(parts) != 10:\n",
    "                print(f\"Warning: Unexpected filename format for {file_name}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, variable_crop_irrigation, region, time_step, start_year, end_year = parts\n",
    "            variable, crop_irrigation = variable_crop_irrigation.split(\"-\", 1)\n",
    "            crop, irrigation = crop_irrigation.rsplit(\"-\", 1)\n",
    "\n",
    "            # Try to find the matching maturity file\n",
    "            key = (model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, crop, irrigation, region, time_step, start_year, end_year)\n",
    "            maturity_data = None\n",
    "\n",
    "            if key in maturity_dict:\n",
    "                ds_maturity = xr.open_dataset(maturity_dict[key], decode_times=False)\n",
    "                maturity_variable_name = list(ds_maturity.data_vars)[0]  # Assuming the first variable is correct\n",
    "                maturity_data = ds_maturity[maturity_variable_name]\n",
    "                ds_maturity.close()\n",
    "\n",
    "            # Append to data list\n",
    "            data.append({\n",
    "                'model': model,\n",
    "                'climate_forcing': climate_forcing,\n",
    "                'climate_scenario': climate_scenario,\n",
    "                'soc_scenario': soc_scenario,\n",
    "                'sens_scenario': sens_scenario,\n",
    "                'variable': variable,\n",
    "                'crop': crop,\n",
    "                'irrigation': irrigation,\n",
    "                'region': region,\n",
    "                'time_step': time_step,\n",
    "                'start_year': start_year,\n",
    "                'end_year': 2016,\n",
    "                'yield_data': yield_data,\n",
    "                'maturity_data': maturity_data  # Include maturity data\n",
    "            })\n",
    "\n",
    "            ds_yield.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    median_df = df.groupby([\n",
    "        'climate_forcing', 'climate_scenario', 'soc_scenario', 'sens_scenario',\n",
    "        'variable', 'crop', 'irrigation', 'region', 'time_step', 'start_year', 'end_year'\n",
    "    ]).agg(\n",
    "        yield_data=('yield_data', lambda x: xr.concat(x, dim=\"model\").median(dim=\"model\")),\n",
    "        maturity_data=('maturity_data', lambda x: xr.concat([d for d in x if d is not None], dim=\"model\").median(dim=\"model\") \n",
    "                    if any(d is not None for d in x) else None)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Add median model entry\n",
    "    median_df['model'] = 'median'\n",
    "\n",
    "    final_df = pd.concat([df, median_df], ignore_index=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "#Load and process data from the ISIMIP3b-YieldModels folder\n",
    "yield_path = r\"C:\\Users\\danie\\OneDrive\\Desktop\\M2 Internship\\Data\\AgriculturalProduction\\ISIMIP3a-Yield\"  \n",
    "maturity_path = r\"C:\\Users\\danie\\OneDrive\\Desktop\\M2 Internship\\Data\\AgriculturalProduction\\ISIMIP3a-MatyDay\"  \n",
    "ISIMIP_Yields_df = process_netcdf_data(yield_path, maturity_path)\n",
    "\n",
    "yield_files = [f for f in os.listdir(yield_path) if f.endswith(\".nc\")]\n",
    "maturity_files = [f for f in os.listdir(maturity_path) if f.endswith(\".nc\")]\n",
    "\n",
    "# Create a dictionary to store parsed metadata of maturity files\n",
    "maturity_dict = {}\n",
    "\n",
    "for file_name in maturity_files:\n",
    "    parts = file_name[:-3].split(\"_\")  # Remove .nc and split\n",
    "    if len(parts) != 10:\n",
    "        continue  # Skip files with incorrect formatting\n",
    "\n",
    "    model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, variable_crop_irrigation, region, time_step, start_year, end_year = parts\n",
    "    variable, crop_irrigation = variable_crop_irrigation.split(\"-\", 1)\n",
    "    crop, irrigation = crop_irrigation.rsplit(\"-\", 1)\n",
    "\n",
    "    # Store the file path using parsed metadata as key\n",
    "    key = (model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, crop, irrigation, region, time_step, start_year, end_year)\n",
    "    maturity_dict[key] = os.path.join(maturity_path, file_name)\n",
    "data = []\n",
    "for file_name in yield_files:\n",
    "\n",
    "        yield_file_path = os.path.join(yield_path, file_name)\n",
    "        ds_yield = xr.open_dataset(yield_file_path, decode_times=False)\n",
    "\n",
    "        # Extract yield variable\n",
    "        variable_names = list(ds_yield.data_vars)\n",
    "        if not variable_names:\n",
    "            print(f\"Warning: No data variables found in {file_name}. Skipping.\")\n",
    "\n",
    "        yield_variable_name = variable_names[0]\n",
    "        yield_data = ds_yield[yield_variable_name]\n",
    "\n",
    "        # Parse filename\n",
    "        parts = file_name[:-3].split(\"_\")\n",
    "        if len(parts) != 10:\n",
    "            print(f\"Warning: Unexpected filename format for {file_name}. Skipping.\")\n",
    "\n",
    "        model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, variable_crop_irrigation, region, time_step, start_year, end_year = parts\n",
    "        variable, crop_irrigation = variable_crop_irrigation.split(\"-\", 1)\n",
    "        crop, irrigation = crop_irrigation.rsplit(\"-\", 1)\n",
    "\n",
    "        # Try to find the matching maturity file\n",
    "        key = (model, climate_forcing, climate_scenario, soc_scenario, sens_scenario, crop, irrigation, region, time_step, start_year, end_year)\n",
    "        maturity_data = None\n",
    "\n",
    "        if key in maturity_dict:\n",
    "            ds_maturity = xr.open_dataset(maturity_dict[key], decode_times=False)\n",
    "            maturity_variable_name = list(ds_maturity.data_vars)[0]  # Assuming the first variable is correct\n",
    "            maturity_data = ds_maturity[maturity_variable_name]\n",
    "            ds_maturity.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate a 365-day time series based on a normal distribution centered on maturity day.\n",
    "\n",
    "Args:\n",
    "    yield_data (xarray.DataArray): The yield for the year.\n",
    "    maturity_day (xarray.DataArray): The day of year when the crop matures.\n",
    "\n",
    "Returns:\n",
    "    xarray.DataArray: A 365-day time series for the year.\n",
    "\"\"\"\n",
    "days = np.arange(0, 365)  # Days of the year\n",
    "\n",
    "# Standard deviation to achieve ~20-day spread\n",
    "std_dev = 20 / 2  # 95% confidence within ~20 days\n",
    "lats,lons = np.where(np.isnan(maturity_data.values).sum(axis=0)<maturity_data.values.shape[0]) #lats, lons are the indices of the pixels that have no missing values   \n",
    "latitudes,longitudes = maturity_data.lat.values[lats],maturity_data.lon.values[lons] #latitudes and longitudes of the pixels that have no missing values\n",
    "pixel_list = np.arange(len(lons))\n",
    "maturitys = np.array([maturity_data.values[:,i,j] for i,j in zip(lats,lons)]) #use the indices to get the maturity data for the pixels that have no missing values\n",
    "yields = np.array([yield_data.values[:,i,j] for i,j in zip(lats,lons)]) #use the indices to get the yield data for the pixels that have no missing values\n",
    "results = []\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def custom_normpdf(x,loc,scale):\n",
    "    '''\n",
    "    Custom normal pdf function that sets the values of the pdf to 0 before and after the maturity day, then normalizes the pdf.   \n",
    "    '''\n",
    "    result = norm.pdf(x, loc=loc, scale=scale)\n",
    "    if ~np.isnan(loc):\n",
    "        loc = int(loc)\n",
    "        min = np.minimum(10,loc).astype(int)\n",
    "        max = np.minimum(10,365-loc).astype(int)\n",
    "        print(loc-min)\n",
    "        result[0:loc-min] = 0\n",
    "        result[loc+max:] = 0\n",
    "        result /= np.sum(result)\n",
    "    return result\n",
    "\n",
    "#@dask.delayed\n",
    "def temporallydownscale_cropyield(maturity_data,yield_data):\n",
    "    '''\n",
    "    Temporally downscales the crop yield data to a daily time series based on the maturity day.\n",
    "    '''\n",
    "    results = np.zeros((365,len(yield_data)),dtype=int)\n",
    "    days = np.arange(1, 366)  # Days of the year\n",
    "    results = [ custom_normpdf(days, loc=maturity_day, scale=10)*yields for maturity_day,yields in zip(maturity_data,yield_data)]\n",
    "    return np.array(results)\n",
    "\n",
    "#\n",
    "#Compute, either with dask or without dask, the temporally downscaled crop yield data for the pixels in the subset_pixel_list\n",
    "#\n",
    "subset_pixel_list = pixel_list[ np.where( (latitudes >= 15) & (latitudes <= 30) & (longitudes >= 15) & (longitudes <= 30) ) ]\n",
    "#africa_pixel_list = pixel_list[ np.where( (latitudes >= -35) & (latitudes <= 37) & (longitudes >= -25) & (longitudes <= 55) ) ]\n",
    "\n",
    "start =time.time()\n",
    "for pixel in subset_pixel_list:\n",
    "    results.append(temporallydownscale_cropyield(maturity_data=maturitys[pixel,-30:-1],yield_data=yields[pixel,-30:-1]))\n",
    "\n",
    "#output = client.compute(results)\n",
    "#final_outputs = np.array([ress.result() for ress in output])\n",
    "final_outputs = np.array(results)\n",
    "end =time.time()\n",
    "print(start-end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(1901, 2016)[-30:-1]  # Reduced above earlier. no need for such a long time series.\n",
    "origin_year = years[0]  # Or any other year you are using.\n",
    "origin_date = pd.to_datetime(f'{origin_year}-01-01')\n",
    "days = (origin_date + pd.to_timedelta(np.arange(365), 'D')).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "##Save the temporally downscaled crop yield data as a netcdf file\n",
    "##\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Generate the latitude and longitude grid\n",
    "# Define years and days\n",
    "years = np.arange(1901, 2016)[-30:-1]  # Reduced above earlier. no need for such a long time series.\n",
    "origin_year = years[0]  # Or any other year you are using.\n",
    "origin_date = pd.to_datetime(f'{origin_year}-01-01')\n",
    "days = (origin_date + pd.to_timedelta(np.arange(365), 'D')).strftime('%Y-%m-%d-%s')\n",
    "#\n",
    "#finally, save as netcdf4 filae\n",
    "#\n",
    "subset_latitudes = latitudes[subset_pixel_list]\n",
    "subset_longitudes = longitudes[subset_pixel_list]\n",
    "#create gridded data\n",
    "yieldts_gridded = np.empty((len(np.unique(subset_latitudes)),len(np.unique(subset_longitudes)),len(years),len(days)))*np.nan\n",
    "for i in range(len(subset_pixel_list)): #length pixel list because that is the spatial dimension collapsed, and we fill time dimensions all at once\n",
    "    yieldts_gridded[np.where(np.unique(subset_latitudes)==subset_latitudes[i])[0][0],np.where(np.unique(subset_longitudes)==subset_longitudes[i])[0][0],:,:] = final_outputs[i,:,:]\n",
    "print('part')\n",
    "# Setting up coordinates and dimensions\n",
    "coords = {\n",
    "    'lat': np.unique(subset_latitudes),\n",
    "    'lon': np.unique(subset_longitudes),\n",
    "    'year': years,\n",
    "    'day': days\n",
    "}\n",
    "dims = ('lat','lon', 'year', 'day')\n",
    "# Creating DataArray\n",
    "data_array = xr.DataArray(yieldts_gridded, coords=coords, dims=dims) #multiply by 100 to convert from fraction to percent.\n",
    "# Creating Dataset\n",
    "dataset = xr.Dataset({\n",
    "    'yield': data_array\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dataset.isel(year=5).drop(\"year\")['yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv for visualization with keplergl\n",
    "dataset.isel(year=5).drop(\"year\")['yield'].fillna(0).to_dataframe().to_csv('yield_ts_1986.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import shapefile\n",
    "\n",
    "def csv_to_shapefile(csv_filepath, shapefile_filepath, lat_field, lon_field, shape_type=shapefile.POINT):\n",
    "    \"\"\"\n",
    "    Converts a CSV file with lat, lon, year, day, yield to a shapefile.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): Path to the input CSV file.\n",
    "        shapefile_filepath (str): Path to the output shapefile (without extension).\n",
    "        lat_field (str): Name of the column containing the latitude coordinates.\n",
    "        lon_field (str): Name of the column containing the longitude coordinates.\n",
    "        shape_type (int): The shape type for the shapefile (e.g., shapefile.POINT).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(csv_filepath, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            fields = reader.fieldnames  # Get field names from the CSV header\n",
    "\n",
    "            # Create a shapefile writer\n",
    "            w = shapefile.Writer(shapefile_filepath, shape_type)\n",
    "\n",
    "            # Add fields to the shapefile (same as the CSV header)\n",
    "            for field_name in fields:\n",
    "                # Assuming all fields are strings (adjust as needed)\n",
    "                if field_name in [lat_field, lon_field]:\n",
    "                    continue #skip lat and lon as they are used for the geometry.\n",
    "                w.field(field_name, 'C', size=254)  # 'C' for character, adjust size as needed\n",
    "\n",
    "            # Iterate through the CSV rows and add points to the shapefile\n",
    "            for i,row in enumerate(reader):\n",
    "                print(i)\n",
    "                try:\n",
    "                    lon = float(row[lon_field])\n",
    "                    lat = float(row[lat_field])\n",
    "                    if shape_type == shapefile.POINT:\n",
    "                        w.point(lon, lat)\n",
    "\n",
    "                    # Add the attributes from the CSV row to the shapefile record, skipping lat and lon\n",
    "                    record_data = [row[field] for field in fields if field not in [lat_field, lon_field]]\n",
    "                    w.record(*record_data)\n",
    "\n",
    "                except ValueError:\n",
    "                    print(f\"Skipping row due to invalid coordinates: {row}\")\n",
    "                except KeyError as e:\n",
    "                    print(f\"Key error: {e}. Check if column names are correct in CSV.\")\n",
    "                    return\n",
    "                except Exception as e:\n",
    "                    print(f\"An unexpected error occurred: {e}. Skipping row.\")\n",
    "\n",
    "            # Save the shapefile\n",
    "            w.close()\n",
    "\n",
    "            print(f\"Shapefile created successfully: {shapefile_filepath}.shp\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: CSV file not found at {csv_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage (replace with your file paths):\n",
    "csv_file = 'yield_ts_2007.csv'  # Replace with your CSV file path\n",
    "shapefile_output = 'yield_data' # Replace with your desired shapefile output name (without .shp)\n",
    "lat_column = 'lat'\n",
    "lon_column = 'lon'\n",
    "\n",
    "# Create a shapefile of points\n",
    "csv_to_shapefile(csv_file, shapefile_output, lat_column, lon_column, shapefile.POINT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
